{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análise Exploratória e Pré-processamento de Dados\n",
        "\n",
        "## Introdução\n",
        "Este notebook tem como objetivo realizar a **Análise Exploratória de Dados (EDA)** e preparar o dataset de solubilidade para a regressão linear que será feita posteriormente. O pipeline de trabalho consiste em:\n",
        "\n",
        "1.  **Carregamento e Configuração do Ambiente;**\n",
        "2.  **Separação de Variáveis;**\n",
        "3.  **Estatísticas Descritivas e Assimetria (Skewness);**\n",
        "4.  **Pré-processamento;**\n",
        "5.  **Análise de Correlações;**\n",
        "6.  **Visualização de Linearidade;**\n",
        "7. **Diagnóstico de variância via screeplot;**\n",
        "8. **Filtragem dos preditores.**\n",
        "\n",
        "\n",
        "Ao final, os dados processados serão salvos para garantir que os notebooks de modelagem utilizem exatamente a mesma base tratada."
      ],
      "metadata": {
        "id": "2l4DUV26eWsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "umkVQVj9Jd6N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import statsmodels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuração visual\n",
        "sns.set_style(\"ticks\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Carregamento e Configuração do Ambiente\n",
        "\n",
        "Nessa parte, definimos os caminhos relativos para garantir que o código funcione em qualquer máquina, desde que a estrutura de diretórios seja mantida. Também carregamos os arquivos `.txt` contendo as features (`X`) e os targets (`y`) para treino e teste."
      ],
      "metadata": {
        "id": "bdrsZgcPeia9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_mtsQA-fJqd4",
        "outputId": "de3060d3-2dbb-49dc-d058-f947b542cd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solTrainX.txt NÃO encontrado em /solubility/solTrainX.txt\n",
            "solTrainY.txt NÃO encontrado em /solubility/solTrainY.txt\n",
            "solTestX.txt NÃO encontrado em /solubility/solTestX.txt\n",
            "solTestY.txt NÃO encontrado em /solubility/solTestY.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/solubility/solTrainX.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3514285009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# carregando por fim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mX_train_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDADOS_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"solTrainX.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDADOS_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"solTrainY.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mX_test_raw\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDADOS_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"solTestX.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/solubility/solTrainX.txt'"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CARREGAMENTO E DADOS\n",
        "# =============================================================================\n",
        "PROJETO_ROOT = Path.cwd().parent  # sobe de notebooks/ para HW2/\n",
        "\n",
        "\n",
        "# adicionar src/ ao sys.path\n",
        "SRC_PATH = PROJETO_ROOT / \"src\"\n",
        "if SRC_PATH.exists():\n",
        "    sys.path.append(str(SRC_PATH))\n",
        "    print(f\"src/ adicionado ao path: {SRC_PATH}\")\n",
        "\n",
        "# carregar os dados da pasta solubility\n",
        "DADOS_PATH = PROJETO_ROOT / \"solubility\"\n",
        "\n",
        "# verificando se os arquivos existem\n",
        "arquivos_esperados = [\"solTrainX.txt\", \"solTrainY.txt\", \"solTestX.txt\", \"solTestY.txt\"]\n",
        "for arquivo in arquivos_esperados:\n",
        "    caminho = DADOS_PATH / arquivo\n",
        "    if caminho.exists():\n",
        "        print(f\"{arquivo} encontrado\")\n",
        "    else:\n",
        "        print(f\"{arquivo} NÃO encontrado em {caminho}\")\n",
        "\n",
        "# carregando por fim\n",
        "X_train_raw = pd.read_csv(DADOS_PATH / \"solTrainX.txt\", sep='\\t')\n",
        "y_train = pd.read_csv(DADOS_PATH / \"solTrainY.txt\", sep='\\t').values.ravel()\n",
        "X_test_raw  = pd.read_csv(DADOS_PATH / \"solTestX.txt\", sep='\\t')\n",
        "y_test  = pd.read_csv(DADOS_PATH / \"solTestY.txt\", sep='\\t').values.ravel()\n",
        "\n",
        "print(f\"\\nDados carregados com sucesso!\")\n",
        "print(f\"X_train: {X_train_raw.shape}, X_test: {X_test_raw.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Separação de Variáveis\n",
        "\n",
        "O dataset contém dois tipos distintos de variáveis:\n",
        "1.  **Binárias (Fingerprints):** Indicam a presença (1) ou ausência (0) de subestruturas moleculares. Elas têm apenas 2 valores únicos. Um exemplo desse tipo de variável pode ser: \"Possui um anel benzênico?\". Visto que essas variáveis são categóricas/indicadoras, transformá-las acabaria com seu significado lógico.\n",
        "\n",
        "2.  **Contínuas:** Representam propriedades físico-químicas como peso molecular, número de átomos, área de superfície, etc.\n",
        "\n",
        "Essa separação é essencial porque aplicaremos transformações (como Yeo-Johnson) **apenas** nas variáveis contínuas."
      ],
      "metadata": {
        "id": "qlWqbr-7eu4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar Binárias e Contínuas\n",
        "cols_binarias = [col for col in X_train_raw.columns if X_train_raw[col].nunique() <= 2]\n",
        "cols_continuas = [col for col in X_train_raw.columns if col not in cols_binarias]\n",
        "\n",
        "# 1. Exibir tipos de variáveis (binárias)\n",
        "print(f\"\\nNúmero de variáveis binárias: {len(cols_binarias)}\")\n",
        "if len(cols_binarias) > 0:\n",
        "    print(f\"Variáveis binárias: {cols_binarias}\")\n",
        "\n",
        "# 2. Exibir tipos de variáveis (contínuas)\n",
        "print(f\"\\nNúmero de variáveis contínuas: {len(cols_continuas)}\")\n",
        "if len(cols_continuas) > 0:\n",
        "    print(f\"Variáveis contínuas: {cols_continuas}\")"
      ],
      "metadata": {
        "id": "zb1xDTyZfBTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Estatísticas Descritivas e Assimetria (Skewness)\n",
        "\n",
        "Antes de modelar, precisamos entender a distribuição dos dados. A tabela abaixo fornece métricas como média, desvio padrão e quartis para as variáveis contínuas.\n",
        "\n",
        "Além disso, calculamos o **skewness (assimetria)**. Modelos lineares (como OLS e Ridge) beneficiam-se de variáveis com distribuição próxima da normal (simétrica). Valores absolutos de skewness altos indicam a necessidade de transformação."
      ],
      "metadata": {
        "id": "Mgy-l7PMfOfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU0ExU2AiUmI"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EDA: VISUALIZAÇÃO INICIAL DOS DADOS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n>>> ANÁLISE DE ESTATÍSTICAS DESCRITIVAS E SKEWNESS\")\n",
        "\n",
        "# 3. Gerar tabela de estatísticas descritivas para variáveis contínuas\n",
        "print(\"\\nEstatísticas Descritivas para Variáveis Contínuas (X_train_raw):\\n\")\n",
        "descriptive_stats = X_train_raw[cols_continuas].describe()\n",
        "print(descriptive_stats.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\n>>> ANALISANDO SKEWNESS DAS VARIÁVEIS CONTÍNUAS...\")\n",
        "\n",
        "# 1. Calcular o skewness (assimetria) de todas as variáveis contínuas em X_train_raw\n",
        "skewness_values = X_train_raw[cols_continuas].skew()\n",
        "\n",
        "# 2. Ordenar os valores de skewness\n",
        "skewness_sorted = skewness_values.sort_values(ascending=False)\n",
        "\n",
        "print(\"Skewness calculado para variáveis contínuas.\")\n",
        "\n",
        "print(\"\\nTop 5 Variáveis Contínuas com Maior Skewness Positivo:\")\n",
        "print(skewness_sorted.head(5).to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\nTop 5 Variáveis Contínuas com Maior Skewness Negativo:\")\n",
        "print(skewness_sorted.tail(5).to_markdown(numalign=\"left\", stralign=\"left\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se pode ver, há vários preditores com valor absoluto da skewness acima de 1 e alguns chegando a passar de 3. Isso indica a necessidade de aplicar o pré-processamento para corrigir a assimetria e colocar os dados na mesma escala."
      ],
      "metadata": {
        "id": "mgjzkVTYqZL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pré-processamento: Transformação Yeo-Johnson\n",
        "\n",
        "Para corrigir a assimetria identificada acima, aplicamos a transformação **Yeo-Johnson**. Ela é similar ao Box-Cox, que foi aplicado no livro-texto, mas suporta valores zero e negativos. A transformação **Yeo-Johnson** busca o parâmetro $\\lambda$ que minimize a assimetria, aproximando a distribuição de uma Gaussiana (Normal).\n",
        "\n",
        "Além disso, após a utilização do **Yeo-Johnson**, também realizamos a padronização dos dados, centralizando a média em 0 e o desvio padrão em 1.\n",
        "\n",
        "Vale ressaltar que o (`.fit`) é aplicado apenas no conjunto de treino. Uma vez tendo achado todos os parâmetros, utilizamos o (`.transform`) no conjunto de teste. Isso garante que nenhuma informação do teste \"vaze\" para o treinamento."
      ],
      "metadata": {
        "id": "ZwIWy9mHfMXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBNLbGtEJm4I"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PRÉ-PROCESSAMENTO (TRANSFORMAÇÃO)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n>>> PRÉ-PROCESSAMENTO (YEO-JOHNSON)\")\n",
        "\n",
        "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "\n",
        "X_train_trans = X_train_raw.copy()\n",
        "X_test_trans = X_test_raw.copy()\n",
        "\n",
        "if len(cols_continuas) > 0:\n",
        "    # Ajusta o transformador APENAS no treino para evitar data leakage\n",
        "    X_train_trans[cols_continuas] = pt.fit_transform(X_train_raw[cols_continuas])\n",
        "    # Aplica a mesma transformação no teste\n",
        "    X_test_trans[cols_continuas] = pt.transform(X_test_raw[cols_continuas])\n",
        "\n",
        "print(\"Variáveis contínuas transformadas e padronizadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Verificação da Eficácia da Transformação\n",
        "\n",
        "Após a transformação, recalculamos o skewness para verificar se ele foi reduzido."
      ],
      "metadata": {
        "id": "IkGQ4r1JfZIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n>>> ANALISANDO SKEWNESS DAS VARIÁVEIS CONTÍNUAS (DEPOIS da Transformação)...\")\n",
        "\n",
        "# Calcular skewness após a transformação\n",
        "skewness_values_trans = X_train_trans[cols_continuas].skew()\n",
        "skewness_sorted_trans = skewness_values_trans.sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nTop 5 Variáveis com Maior Skewness Positivo (Pós-Transformação):\")\n",
        "print(skewness_sorted_trans.head(5).to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\nTop 5 Variáveis com Maior Skewness Negativo (Pós-Transformação):\")\n",
        "print(skewness_sorted_trans.tail(5).to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "id": "LJSIsRelLH96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pode-se ver que, apesar de ter algumas variáveis com **skewness** ainda acima de 1 e até de 2, os valores em geral diminuiram de forma significativa e a maioria agora possui **skewness** próximo de 0. Com isso, os dados estão mais preparados para realização da regressão.\n",
        "\n",
        "Agora, vamos analisar o comportamento dos preditores com a variável alvo em alguns aspectos."
      ],
      "metadata": {
        "id": "NRqOQu6gtHKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Análise de Correlações\n",
        "\n",
        "Nessa célula, investigamos como as variáveis contínuas se relacionam com a variável alvo (`Solubility`). Calculamos a correlação de Pearson e exibimos as variáveis mais correlacionadas (tanto positiva quanto negativamente).\n",
        "\n",
        "A correlação de Pearson mede a força e a direção da relação linear entre duas variáveis, variando de -1 a 1."
      ],
      "metadata": {
        "id": "yjzX5qC3fef3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JvkTrLeJtBw"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EDA: CORRELAÇÕES\n",
        "# =============================================================================\n",
        "print(\"\\n>>> ANÁLISE DE CORRELAÇÕES (Parte 0)...\")\n",
        "\n",
        "# 1. Matriz de Correlação entre Preditores\n",
        "\n",
        "# Nesse caso, pegamos só as contínuas para não deixar o mapa de calor muito grande\n",
        "corr_matrix = X_train_trans[cols_continuas].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, square=True, cbar_kws={\"shrink\": .5})\n",
        "plt.title(\"Matriz de Correlação (Preditores Contínuos)\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Correlação Linear Individual (Preditores vs Outcome)\n",
        "y_train_series = pd.Series(y_train, index=X_train_trans.index)\n",
        "\n",
        "correlations_y = X_train_trans.corrwith(y_train_series).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "print(\"\\nTop 5 Correlações Positivas com Solubilidade:\")\n",
        "print(correlations_y.head(5))\n",
        "print(\"\\nTop 5 Correlações Negativas com Solubilidade:\")\n",
        "print(correlations_y.tail(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O mapa de calor acima mostra a correlação entre os preditores. Regiões vermelhas ou azuis escuras fora da diagonal principal indicam que duas variáveis explicam praticamente a mesma coisa. Isso é problemático para modelos lineares, pois infla a variância dos coeficientes, tornando o modelo instável. Essa multicolinearidade será tratada em breve."
      ],
      "metadata": {
        "id": "kzD6BMcNvJcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualização de Linearidade\n",
        "\n",
        "Uma vez tendo visto quais variáveis mais/menos se correlacionam com a variável alvo (`Solubility`), vamos agora visualizar também como é comportamento de cada variável com a variável alvo utilizando scatter plots com uma linha de suavização. Dessa forma, será possível verificar a linearidade entre os preditores e o alvo, podendo assim, identificar possíveis problemas para a aplicação da regressão, uma vez que o modelo linear falhará em capturar relações que não são lineares, resultando em erros sistemáticos.\n"
      ],
      "metadata": {
        "id": "Esj3uMtifjo6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdbAvNboqSZY"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZAÇÃO DE LINEARIDADE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n>>> GERANDO SCATTER PLOTS (Predictor vs Outcome)\")\n",
        "\n",
        "# Vamos pegar as variáveis contínuas transformadas\n",
        "\n",
        "num_vars = len(cols_continuas)\n",
        "cols = 4\n",
        "rows = (num_vars // cols) + 1\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(cols_continuas):\n",
        "    # Plot scatter com linha de suavização (lowess)\n",
        "    sns.regplot(x=X_train_trans[col], y=y_train, ax=axes[i],\n",
        "                lowess=True, # Isso faz a linha curva suave (não linear)\n",
        "                scatter_kws={'alpha': 0.3, 's': 10, 'color': 'gray'},\n",
        "                line_kws={'color': 'red'})\n",
        "    axes[i].set_title(col)\n",
        "    axes[i].set_xlabel(\"Transformed Value\")\n",
        "    axes[i].set_ylabel(\"Solubility\")\n",
        "\n",
        "# Remove eixos vazios se houver\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Comentário: Observe se a linha vermelha é reta (linear) ou curva (não-linear).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apesar de alguns preditores possuirem relações não-lineares com a variável alvo, o comportamento no geral é mais linear."
      ],
      "metadata": {
        "id": "mwlSfnZRwbdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Diagnóstico de variância via screeplot\n",
        "\n",
        "Assim como nas duas células anteriores, mais uma vez essa célula serve com o intuito de analisar o comportamento dos preditores no dataset, sem alterá-lo. Dessa vez, o objetivo é visualizar o Scree Plot dos preditores, a partir da PCA, e entender a partipação de cada variável na informação total dos dados, podendo assim descobrir quantos componentes principais seriam suficientes para representar a maior parte da informação original dos dados ao mesmo tempo que ajuda a entender a redundância da informação. Tal informação também contribui para a redução a dimensionalidade e a diminuição da multicolineariade, que é péssima para a realização da regressão linear, principalmente para aquela feita por Mínimos Quadrados Ordinários (OLS), uma vez que torna a matriz $X^T X$ quase singular (não invertível).\n",
        "\n"
      ],
      "metadata": {
        "id": "Xrm4HqQBfpor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "525hMj-qq9rO"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 3.4. DIAGNÓSTICO DE VARIÂNCIA VIA PCA - SCREE PLOT\n",
        "# =============================================================================\n",
        "print(\"\\n>>> GERANDO SCREE PLOT (PCA)...\")\n",
        "\n",
        "# Para a visualização da PCA (e apenas para ela), precisamos\n",
        "# colocar tudo na mesma escala para fazer o gráfico\n",
        "# Isso não afeta os dados X_train_trans usados na regressão depois.\n",
        "\n",
        "# 1. Cria uma cópia temporária para não estragar o dataset principal\n",
        "X_pca_temp = X_train_trans.copy()\n",
        "\n",
        "# 2. Padroniza tudo (Binárias + Contínuas) apenas para o PCA\n",
        "scaler_global = StandardScaler()\n",
        "X_pca_scaled = scaler_global.fit_transform(X_pca_temp)\n",
        "\n",
        "# 3. Roda o PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_pca_scaled)\n",
        "\n",
        "# 4. Plota\n",
        "explained_var = pca.explained_variance_ratio_ * 100\n",
        "components = np.arange(1, len(explained_var) + 1)\n",
        "\n",
        "print(f\"Variância explicada pelo 1º Componente: {explained_var[0]:.2f}%\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(components, explained_var, 'b-', linewidth=1.5)\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel(\"Componente Principal\")\n",
        "plt.ylabel(\"Variância Explicada (%)\")\n",
        "plt.xlim(0, 200)\n",
        "plt.ylim(0, 25)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o scree plot, foi possível ver que a variável que mais possui informação, explica menos de 13% da variância total. Além disso, observa-se que a partir de uma determinada quantidade de variáveis, praticamente toda a variância foi explicada, o que confirma que há redundância (multicolinearidade) nos dados originais."
      ],
      "metadata": {
        "id": "y4e-5ukb7vbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Filtragem dos preditores\n",
        "\n",
        "Uma vez tendo analisado as correlações do preditores com a variável alvo, a linearidade e variância com scree plot, vamos combater a multicolinearidade removendo qualquer preditor que possua correlação maior que 0.9 com outro preditor.\n",
        "\n",
        "Em outras palavras, definimos um limiar de corte (*threshold*) de **0.9**. O algoritmo percorre a matriz de correlação e, ao encontrar um par de variáveis com $|r| > 0.9$, remove uma delas. Isso simplifica o modelo, mantendo a maior parte da informação única.\n",
        "\n",
        "No livro-texto, foi utilizada uma função em R que possui uma maneira um pouco diferente daquela feita aqui, podendo causar uma pequena diferença na quantidade de variáveis removidas.\n",
        "\n"
      ],
      "metadata": {
        "id": "xTLCPVixfxMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COKhXD24JxMJ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FILTRAGEM DE PREDIRORES (Alta Colinearidade)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n>>> REMOVENDO ALTA CORRELAÇÃO (>0.9)...\")\n",
        "\n",
        "def identificar_correlacoes(df, threshold=0.9):\n",
        "    corr_matrix = df.corr().abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "    return to_drop\n",
        "\n",
        "cols_drop = identificar_correlacoes(X_train_trans, 0.9)\n",
        "X_train_filtered = X_train_trans.drop(columns=cols_drop)\n",
        "X_test_filtered = X_test_trans.drop(columns=cols_drop)\n",
        "\n",
        "print(f\"Preditores iniciais: {X_train_trans.shape[1]}\")\n",
        "print(f\"Preditores removidos: {len(cols_drop)}\")\n",
        "print(f\"Preditores finais: {X_train_filtered.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Persistência dos Dados\n",
        "\n",
        "Ao final disso tudo, possuimos o dataset com os dados processados. Sendo assim, ele está pronto para a aplicação do próximo passo, que no caso é a OLS. Isso será visto no arquivo (`analise_OLS.ipynb`).\n",
        "\n",
        "Salvamos os dados transformados em um arquivo `pickle` para que possam ser carregados rapidamente nos notebooks de modelagem, garantindo consistência em todo o projeto."
      ],
      "metadata": {
        "id": "Ci4r78OUf19n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImWhrhhLIwbr"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"dados_preprocessados.pkl\", \"wb\") as f:\n",
        "    pickle.dump(\n",
        "        (X_train_filtered, X_test_filtered, y_train, y_test),\n",
        "        f\n",
        "    )\n",
        "\n",
        "print(\"Dados pré-processados salvos com sucesso!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
